# np_input = False,
# center_crop = center_crop,
# crop_size = crop_size,
# scale_range = crop_scale_range,
# horizontal_flip_probability = kwargs.get('horizontal_flip_probability', 0.5),
# color_jitter = True,
# brightness_jitter = kwargs.get('brightness_jitter', 0.8), # 0.8
# contrast_jitter = kwargs.get('contrast_jitter', 0.8), # 0.8
# saturation_jitter = kwargs.get('saturation_jitter', 0.0),
# hue_jitter = kwargs.get('hue_jitter', 0.0),
# color_jitter_probability = kwargs.get('color_jitter_probability', 0.1), # 0.8
# random_grayscale = False,
# random_gaussian_blur = True,
# gaussian_blur_sigma = kwargs.get('gaussian_blur_sigma', (0.01, 4.0)),
# gaussian_blur_probability = kwargs.get('gaussian_blur_probability_global_1', 0.5),
# random_solarize = False,
# random_rotation = True,
# rotation_degree = (kwargs.get('rotation_degree', (-180., 180.))),
# rotation_probability = (kwargs.get('rotation_probability', (0.1))),
# normalize = True,
# mean = dataset_mean,
# std = dataset_std,

experiment:
  seed: 12
  device: cuda 
  expt_name: simsiam_vit_small # experiment name
  log_freq: 20 # knn accuracy is computed every `log_freq` epochs, additionally the trained model is also saved
  output_dir: ../working/1407/simsiam_vit_small # output results to this folder - files: logs.txt, {experiment_name}_models
  ssl_training: simsiam # training strategy to use
  use_mixed_precision: true # if true uses fp16 for training, improves training speed
# input image parameters
input:
  channels: 3 # num channels in input image
  data path: ../input/real_lenses_dataset # path/to/dataset
  indices: ../input/indices.pkl 
  image size: 
  - 64 # image height to perform center crop
  - 64 # image width to perform center crop
  num classes: 2
# training network parameters
network:
  backbone: vit_small # backbone network
  projector_head_hidden_dim: 512 
  head_output_dim: 2048
  projector_head_nlayers: 3
  projector_use_bn: true
  predictor_head_hidden_dim: 512
  patch_size: 16
optimizer:
  init_lr: 0.05 # anything more than this is unstable
  init_wd: 0.0001 # low momentum helps, zero doesn't give better results
  final_lr: 0.0005 # keeping this low helps, too low makes learning slow
  final_wd: 0.0001 # high momentum at the end regularizes
  optimizer: SGD
  scheduler_warmup_epochs: 0
  clip_grad_magnitude: 0. # check
# restart training from checkpoint file ckpt
restore:
  ckpt_path: null # path/to/ckpt
  restore: false # if true, restart from provided ckpt
# keyword args for the ssl per data augmentation
ssl augmentation kwargs: 
  augmentation: AugmentationSIMSIAM
  center_crop: 64
  crop_scale: 
  - 0.14
  - 1.0
  crop_size: 64
# training parameters
train args:
  knn_neighbours: 20
  batch_size: 512
  num_epochs: 100
  freeze_last_layer: 0
  train_val_split: 
  - 0.85
  - 0.15